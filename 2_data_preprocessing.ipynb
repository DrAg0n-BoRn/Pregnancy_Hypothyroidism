{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import PM\n",
    "import polars as pl\n",
    "from helpers.constants import ID_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised version on 2025-02-21, 2025-03-20, and 2026-01-04\n",
    "clean_data_path = PM.clean_data_cn_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fix = {\n",
    "    \"38-40周血葡萄糖\": pl.Utf8,\n",
    "    \"30.1-37.6周血葡萄糖\": pl.Utf8,\n",
    "    \"10.1-20周促甲状腺激素受体抗体\": pl.Utf8,\n",
    "    \"21-40周促甲状腺激素受体抗体\": pl.Utf8,\n",
    "    \"25.1-32w_脐动脉S/D\": pl.Utf8,\n",
    "    \"18.1-25w_脐动脉RI\": pl.Utf8,\n",
    "    \"1-10周促甲状腺激素受体抗体\": pl.Utf8,\n",
    "    \"1-11.6周糖化血红蛋白\": pl.Utf8,\n",
    "    \"25.1-33.6周γ-谷氨酰转肽酶\": pl.Utf8,\n",
    "    \"22.1-32周乳酸脱氢酶\": pl.Utf8,\n",
    "    \"25.1-33.6周谷丙转氨酶\": pl.Utf8,\n",
    "    \"25.1-33.6周谷草转氨酶\": pl.Utf8,\n",
    "    \"1-10周TGAB\": pl.Utf8,\n",
    "    \"25.1-32w_羊水深度\": pl.Utf8,\n",
    "    \"1-12.6周谷丙转氨酶\": pl.Utf8,\n",
    "    \"18.1-25w_双顶径\": pl.Utf8,\n",
    "    \"13-25周γ-谷氨酰转肽酶\": pl.Utf8,\n",
    "    \"1-22周乳酸脱氢酶\": pl.Utf8,\n",
    "    \"13-25周谷丙转氨酶\": pl.Utf8,\n",
    "    \"13-25周谷草转氨酶\": pl.Utf8,\n",
    "    \"38-40周谷丙转氨酶\": pl.Utf8,\n",
    "    \"32.1-40周乳酸脱氢酶\": pl.Utf8,\n",
    "} # Obtained by trial and error\n",
    "\n",
    "df = pl.read_csv(clean_data_path, infer_schema=True, schema_overrides=columns_to_fix)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for manually identified columns\n",
    "for col in columns_to_fix.keys():\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column to fix '{col}' not found in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into two dataframes: one with columns to fix and the rest\n",
    "df_ok = df.drop(list(columns_to_fix.keys()))\n",
    "\n",
    "df_to_fix = df.select([ID_COL] + list(columns_to_fix.keys()))\n",
    "\n",
    "print(df_ok.shape, df_to_fix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Delete features with more empty values than a given threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_columns_with_nulls(df: pl.DataFrame, threshold_percentage: float=0.5):\n",
    "    threshold = threshold_percentage * df.height\n",
    "\n",
    "    # Get null counts for all columns in a single operation\n",
    "    null_counts = df.null_count()\n",
    "\n",
    "    # Create a Series mapping column names to their null counts\n",
    "    null_counts_dict = dict(zip(null_counts.columns, null_counts.row(0)))\n",
    "\n",
    "    # Identify columns to drop\n",
    "    cols_to_drop = [col for col, null_count in null_counts_dict.items() if null_count >= threshold]\n",
    "\n",
    "    # Drop columns iand return the new DataFrame\n",
    "    return df.drop(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok_2 = delete_columns_with_nulls(df_ok)\n",
    "df_to_fix_2 = delete_columns_with_nulls(df_to_fix)\n",
    "print(df_ok_2.shape, df_to_fix_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cast and amend data entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cast_value(value, column_name, row_idx):\n",
    "    '''\n",
    "    Try to cast a value to an integer or float. If it fails, prompt the user to enter a corrected value or set it as null.\n",
    "    '''\n",
    "    while True:\n",
    "        try:\n",
    "            return int(value)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return float(value)\n",
    "            except ValueError:\n",
    "                user_input = input(\n",
    "                    f\"Error casting value '{value}' in column '{column_name}', row {row_idx}. \"\n",
    "                    f\"Enter a corrected value or type 'None'/'Null' to set it as null: \"\n",
    "                ).strip()\n",
    "                \n",
    "                if user_input.lower() in [\"none\", \"null\"]:\n",
    "                    return None\n",
    "                else:\n",
    "                    value = user_input  # Update the value and retry casting\n",
    "                    \n",
    "\n",
    "def fix_values_dtype(df: pl.DataFrame):\n",
    "    # Fix columns\n",
    "    processed_columns = dict()\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column == ID_COL:\n",
    "            processed_columns[column] = df[column].to_list()\n",
    "            continue\n",
    "        \n",
    "        processed_values = []\n",
    "        for row_idx, value in enumerate(df[column].to_list(), start=1):\n",
    "            if value is None:\n",
    "                processed_values.append(None)\n",
    "                continue\n",
    "            processed_value = _cast_value(value, column, row_idx)\n",
    "            processed_values.append(processed_value)\n",
    "        \n",
    "        # Check if conversion to all floats is needed (avoid integers and float mixture)\n",
    "        has_floats = any(isinstance(v, float) for v in processed_values)\n",
    "        if has_floats:\n",
    "            processed_values = [\n",
    "                float(v) if v is not None else None \n",
    "                for v in processed_values\n",
    "            ]\n",
    "        \n",
    "        processed_columns[column] = processed_values\n",
    "\n",
    "    # Create a new DataFrame with the processed data\n",
    "    return pl.DataFrame(processed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_fix_3 = fix_values_dtype(df_to_fix_2)\n",
    "print(df_to_fix_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate row counts\n",
    "print(df_ok_2.shape, df_to_fix_3.shape)\n",
    "assert df_ok_2.height == df_to_fix_3.height, \"Row counts do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two DataFrames on ID\n",
    "merged_df = df_ok_2.join(df_to_fix_3, on=ID_COL, how=\"inner\")\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Fix Faulty Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix faulty entries in the \"身高\" column. If the data value is less than 150, set it to empty. \n",
    "merged_df = merged_df.with_columns(\n",
    "    pl.when(pl.col(\"身高\") < 150.0)\n",
    "    .then(None)\n",
    "    .otherwise(pl.col(\"身高\"))\n",
    "    .alias(\"身高\")\n",
    ")\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Audit translation for column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_tools.utilities import audit_column_translation, save_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_column_translation(df_or_path=merged_df, mapper=PM.translation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(df=merged_df, full_path=PM.processed_data_cn_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pregnancy-hypothyroidism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
