{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_tools.ML_datasetmaster import DragonDataset\n",
    "from ml_tools.ML_models import DragonNodeModel\n",
    "from ml_tools.ML_configuration import (\n",
    "    FormatBinaryClassificationMetrics,\n",
    "    FinalizeBinaryClassification,\n",
    "    DragonNodeParams\n",
    ")\n",
    "\n",
    "from ml_tools.ML_configuration import DragonTrainingConfig\n",
    "from ml_tools.ML_trainer import DragonTrainer\n",
    "from ml_tools.ML_callbacks import DragonModelCheckpoint, DragonPatienceEarlyStopping, DragonPlateauScheduler\n",
    "from ml_tools.ML_utilities import build_optimizer_params, inspect_model_architecture\n",
    "from ml_tools.utilities import load_dataframe_with_schema\n",
    "from ml_tools.path_manager import sanitize_filename\n",
    "from ml_tools.IO_tools import train_logger\n",
    "from ml_tools.schema import FeatureSchema\n",
    "from ml_tools.keys import TaskKeys\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from paths import PM\n",
    "\n",
    "# Choose Target\n",
    "from helpers.constants import TARGET_AMNIOTIC_FLUID_CONTAMINATION as TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "sanitized_target = sanitize_filename(TARGET)\n",
    "\n",
    "PM.train_results = PM.results / sanitized_target\n",
    "PM.train_artifacts = PM.train_results / \"Artifacts\"\n",
    "PM.train_checkpoints = PM.train_results / \"Checkpoints\"\n",
    "PM.train_evaluation = PM.train_results / \"Evaluation\"\n",
    "\n",
    "PM.make_dirs()\n",
    "\n",
    "# Local path constants\n",
    "SCHEMA_PATH = PM.engineering_artifacts\n",
    "TRAIN_DATASET_FILE = PM.train_datasets / (sanitized_target + '.csv')\n",
    "TRAIN_ARTIFACTS_DIR = PM.train_artifacts\n",
    "TRAIN_CHECKPOINTS_DIR = PM.train_checkpoints\n",
    "TRAIN_EVALUATION_DIR = PM.train_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = DragonTrainingConfig(\n",
    "    validation_size=0.2,\n",
    "    test_size=0.1,\n",
    "    initial_learning_rate=0.00005,\n",
    "    batch_size=24,\n",
    "    task = TaskKeys.BINARY_CLASSIFICATION,\n",
    "    device = \"cuda:0\",\n",
    "    finalized_filename = f\"node_{sanitized_target}\",\n",
    "    random_state=101,\n",
    "    \n",
    "    target=TARGET,\n",
    "    early_stop_patience=20,\n",
    "    scheduler_patience=3,\n",
    "    scheduler_lr_factor=0.5,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Load Schema and Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = FeatureSchema.from_json(SCHEMA_PATH)\n",
    "\n",
    "df, _ = load_dataframe_with_schema(df_path=TRAIN_DATASET_FILE, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Make Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DragonDataset(pandas_df=df,\n",
    "                        schema=schema,\n",
    "                        kind=train_config.task,\n",
    "                        feature_scaler=\"fit\",\n",
    "                        target_scaler=\"none\",\n",
    "                        validation_size=train_config.validation_size,\n",
    "                        test_size=train_config.test_size,\n",
    "                        random_state=train_config.random_state,\n",
    "                        class_map={\"Negative\": 0, \"Positive\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = DragonNodeParams(\n",
    "    schema=schema,\n",
    "    out_targets=1,\n",
    "    embedding_dim=32,\n",
    "    num_trees=1024,\n",
    "    num_layers=2,\n",
    "    tree_depth=5,\n",
    "    additional_tree_output_dim=3,\n",
    "    input_dropout=0.0,\n",
    "    embedding_dropout=0.0,\n",
    "    choice_function='sparsemax',\n",
    "    bin_function='sparsemoid',\n",
    "    batch_norm_continuous=False\n",
    ")\n",
    "\n",
    "model = DragonNodeModel(**model_params)\n",
    "# Initialize decision thresholds before training.\n",
    "model.data_aware_initialization(train_dataset=dataset.train_dataset, num_samples=500)\n",
    "\n",
    "# optimizer\n",
    "optim_params = build_optimizer_params(model=model, weight_decay=0.001)\n",
    "optimizer = AdamW(params=optim_params, lr=train_config.initial_learning_rate)\n",
    "\n",
    "trainer = DragonTrainer(model=model,\n",
    "                        train_dataset=dataset.train_dataset,\n",
    "                        validation_dataset=dataset.validation_dataset,\n",
    "                        kind=train_config.task,\n",
    "                        optimizer=optimizer,\n",
    "                        device=train_config.device,\n",
    "                        checkpoint_callback=DragonModelCheckpoint(save_dir=TRAIN_CHECKPOINTS_DIR, \n",
    "                                                                  monitor=\"Validation Loss\"),\n",
    "                        early_stopping_callback=DragonPatienceEarlyStopping(patience=train_config.early_stop_patience, \n",
    "                                                                            monitor=\"Validation Loss\"),\n",
    "                        lr_scheduler_callback=DragonPlateauScheduler(monitor=\"Validation Loss\",\n",
    "                                                                     patience=train_config.scheduler_patience,\n",
    "                                                                     factor=train_config.scheduler_lr_factor),  \n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.fit(save_dir=TRAIN_ARTIFACTS_DIR, epochs=500, batch_size=train_config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(save_dir=TRAIN_EVALUATION_DIR,\n",
    "                model_checkpoint=\"best\",\n",
    "                test_data=dataset.test_dataset,\n",
    "                classification_threshold=0.5,\n",
    "                val_format_configuration=FormatBinaryClassificationMetrics(cmap=\"BuGn\", ROC_PR_line=\"darkorange\"),\n",
    "                test_format_configuration=FormatBinaryClassificationMetrics(cmap=\"Purples\", ROC_PR_line=\"tab:pink\"),\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 7. Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.explain_captum(save_dir=TRAIN_EVALUATION_DIR,\n",
    "                       n_samples=200,\n",
    "                       n_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 8. Save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset artifacts\n",
    "dataset.save_artifacts(TRAIN_ARTIFACTS_DIR)\n",
    "\n",
    "# Model artifacts\n",
    "model.save_architecture(TRAIN_ARTIFACTS_DIR)\n",
    "inspect_model_architecture(model=model, save_dir=TRAIN_ARTIFACTS_DIR)\n",
    "\n",
    "# FeatureSchema\n",
    "schema.to_json(TRAIN_ARTIFACTS_DIR)\n",
    "\n",
    "# Train log\n",
    "train_logger(train_config=train_config,\n",
    "             model_parameters=model_params,\n",
    "             train_history=history,\n",
    "             save_directory=TRAIN_ARTIFACTS_DIR.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 9. Finalize Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.finalize_model_training(model_checkpoint='current',\n",
    "                                save_dir=TRAIN_ARTIFACTS_DIR,\n",
    "                                finalize_config=FinalizeBinaryClassification(filename=train_config.finalized_filename,\n",
    "                                                                            target_name=dataset.target_names[0],\n",
    "                                                                            classification_threshold=0.50,\n",
    "                                                                            class_map=dataset.class_map))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pregnancy-hypothyroidism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
